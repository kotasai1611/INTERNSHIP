{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2KNYwpR480CO"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score , roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from textblob import TextBlob\n",
        "import spacy , contractions\n",
        "import nltk , re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import joblib\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization , LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "w01U0GIx-C-U"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('Copy of prediction_data.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BjeuZEYU-DKt"
      },
      "outputs": [],
      "source": [
        "df.drop(columns = ['Unnamed: 0.1' , 'Unnamed: 0' , 'num_words_in_transcript'] , inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXsoj-5J-DN3",
        "outputId": "3d2683ef-afd6-4aef-c2e8-b2f2be25fc5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 7 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   ID                   100 non-null    object\n",
            " 1   Name                 100 non-null    object\n",
            " 2   Role                 100 non-null    object\n",
            " 3   Transcript           100 non-null    object\n",
            " 4   Resume               100 non-null    object\n",
            " 5   Reason for decision  100 non-null    object\n",
            " 6   Job Description      100 non-null    object\n",
            "dtypes: object(7)\n",
            "memory usage: 5.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9my-ZD9BWaU",
        "outputId": "259ed26f-431f-47d6-c3a3-0054ed46349a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\heman\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "oNyq4dw0_Bdd"
      },
      "outputs": [],
      "source": [
        "class Preprocessing:\n",
        "\n",
        "  def __init__(self , data):\n",
        "    self.data = data\n",
        "    print('Initiating the Pre processing \\n')\n",
        "    self.textProcessing()\n",
        "\n",
        "  def textProcessing(self):\n",
        "\n",
        "    print('Step 1/8 ==> Started ............')\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    stopword_s = stopwords.words('english')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def pre_process(text):\n",
        "      text = contractions.fix(text) # expanding the contractions like , I'm with I am and it's with It is and so on...\n",
        "      text = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@\\[\\]^_`{|}~]', '', text)  # Removing all these characters from the text\n",
        "      sentences = re.split(r'(?<=[.!?])\\s+', text)   # Splitting the text as sentences using the regular expressions\n",
        "      for idx, sent in enumerate(sentences):\n",
        "          words = nlp(sent)  # converting the sentence into words\n",
        "          words = [word.text for word in words if word.text.lower() not in stopword_s] # Removing the stopwords from the text\n",
        "          words = [lemmatizer.lemmatize(word) for word in words]  # Using the Lemmatization techniques\n",
        "          sentences[idx] = ' '.join(words)  # Replacing the original sentence with the new sentence\n",
        "      return ' '.join(sentences)  # Join the sentences back together\n",
        "\n",
        "    self.data['Resume_processed'] = self.data['Resume'].apply(pre_process)\n",
        "    self.data['Job_Description_processed'] = self.data['Job Description'].apply(pre_process)\n",
        "    self.data['Transcript_processed'] = self.data['Transcript'].apply(pre_process)\n",
        "\n",
        "    print('Step 1/8 ==> Pre processing of texts in the dataset executed Successfully \\n')\n",
        "\n",
        "    self.analyze_sentiment()\n",
        "\n",
        "  def analyze_sentiment(self):\n",
        "\n",
        "    print('Step 2/8 ==> Started ............')\n",
        "\n",
        "    def sentiment(text):\n",
        "      # Create a TextBlob object\n",
        "      blob = TextBlob(text)\n",
        "\n",
        "      # Get the sentiment polarity\n",
        "      polarity = blob.sentiment.polarity\n",
        "\n",
        "      return polarity\n",
        "\n",
        "    self.data['Transcript_sentiment'] = self.data['Transcript_processed'].apply(sentiment)\n",
        "    self.data['Resume_sentiment'] = self.data['Resume_processed'].apply(sentiment)\n",
        "    self.data['JobDescription_sentiment'] = self.data['Job_Description_processed'].apply(sentiment)\n",
        "\n",
        "    print('Step 2/8 ==> Sentiment Calculation of Each colum Excuted Succesfully \\n')\n",
        "\n",
        "    self.word_counts()\n",
        "\n",
        "  def word_counts(self):\n",
        "\n",
        "    print('Step 3/8 ==> Started ............')\n",
        "\n",
        "    def counts(text):\n",
        "      return len(text.split())\n",
        "\n",
        "    self.data['Transcript_words'] = self.data['Transcript_processed'].apply(counts)\n",
        "    self.data['Resume_words'] = self.data['Resume_processed'].apply(counts)\n",
        "\n",
        "    print('Step 3/8 ==> Extracting Word Counts Excuted Succesfully \\n')\n",
        "\n",
        "    self.extract_years_of_experience()\n",
        "\n",
        "  def extract_years_of_experience(self):\n",
        "\n",
        "      print('Step 4/8 ==> Started ............')\n",
        "\n",
        "      def years_of_experience(text):\n",
        "          # Regular expression to extract years and date ranges\n",
        "          date_ranges = re.findall(r\"(\\d{4})-(present|\\d{4})\", text.lower())\n",
        "\n",
        "          total_years = 0\n",
        "          current_year = datetime.now().year\n",
        "\n",
        "          for start, end in date_ranges:\n",
        "              start_year = int(start)\n",
        "              end_year = current_year if end == \"present\" else int(end)\n",
        "\n",
        "              total_years += (end_year - start_year)\n",
        "\n",
        "          return total_years\n",
        "      self.data['Years_Experience']=self.data['Resume'].apply(years_of_experience)\n",
        "\n",
        "      print('Step 4/8 ==> Extracting Years of Experience Excuted Succesfully \\n')\n",
        "      self.process_dataset()\n",
        "\n",
        "\n",
        "  def extract_resume_features(self,resume_text, jd_text):\n",
        "    features = {}\n",
        "\n",
        "    # Length-based features\n",
        "    features['resume_sentence_count'] = len(sent_tokenize(resume_text))\n",
        "    features['resume_avg_word_length'] = sum(len(word) for word in word_tokenize(resume_text)) / len(word_tokenize(resume_text))\n",
        "\n",
        "    # Skill match features\n",
        "    jd_skills = set(re.findall(r'\\b[A-Za-z]+\\b', jd_text.lower()))\n",
        "    resume_skills = set(re.findall(r'\\b[A-Za-z]+\\b', resume_text.lower()))\n",
        "    features['skill_match_count'] = len(jd_skills & resume_skills)\n",
        "\n",
        "    # Education features\n",
        "    degrees = ['b.tech', 'm.tech', 'mba', 'phd', 'bachelor', 'master']\n",
        "    features['university_education_count'] = sum(1 for degree in degrees if degree in resume_text.lower())\n",
        "\n",
        "    return features\n",
        "\n",
        "  def process_dataset(self):\n",
        "\n",
        "    print('Step 5/8 ==> Started ............')\n",
        "\n",
        "    def process_dataset_resume_features(data, resume_col, jd_col):\n",
        "        \"\"\"\n",
        "        Process a dataset to extract features for each resume-JD pair.\n",
        "\n",
        "        Parameters:\n",
        "        - data (pd.DataFrame): Dataset containing resumes and job descriptions.\n",
        "        - resume_col (str): Column name for resumes.\n",
        "        - jd_col (str): Column name for job descriptions.\n",
        "\n",
        "        Returns:\n",
        "        - pd.DataFrame: Original dataset with extracted features appended.\n",
        "        \"\"\"\n",
        "        extracted_features = []\n",
        "\n",
        "        for _, row in data.iterrows():\n",
        "            resume_text = row[resume_col]\n",
        "            jd_text = row[jd_col]\n",
        "            features = self.extract_resume_features(resume_text, jd_text)\n",
        "            extracted_features.append(features)\n",
        "\n",
        "        # Combine features into a DataFrame\n",
        "        features_df = pd.DataFrame(extracted_features)\n",
        "        return pd.concat([data, features_df], axis=1)\n",
        "\n",
        "    # Process the dataset\n",
        "    self.processed_data = process_dataset_resume_features(self.data, \"Resume_processed\", \"Job_Description_processed\")\n",
        "\n",
        "    print('Step 5/8 ==> Extracting Resume Features Excuted Succesfully \\n')\n",
        "\n",
        "    self.process_transcript()\n",
        "\n",
        "  def extract_transcript_features(self,transcript_text):\n",
        "\n",
        "    features = {}\n",
        "    # Language features\n",
        "    features['transcript_vocab_diversity'] = len(set(word_tokenize(transcript_text))) / len(word_tokenize(transcript_text))\n",
        "    features['transcript_avg_sentence_length'] = sum(len(sent.split()) for sent in sent_tokenize(transcript_text)) / len(sent_tokenize(transcript_text))\n",
        "    return features\n",
        "\n",
        "  def process_transcript(self):\n",
        "\n",
        "    print('Step 6/8 ==> Started ............')\n",
        "    def process_transcript(data, transcript_col = 'Transcript_processed'):\n",
        "      extracted_features = []\n",
        "\n",
        "      for _, row in data.iterrows():\n",
        "          transcript_text = row[transcript_col]\n",
        "          features = self.extract_transcript_features(transcript_text)\n",
        "          extracted_features.append(features)\n",
        "\n",
        "      # Combine features into a DataFrame\n",
        "      features_df = pd.DataFrame(extracted_features)\n",
        "      return pd.concat([data, features_df], axis=1)\n",
        "\n",
        "     # Process the dataset\n",
        "    self.processed_data = process_transcript(self.processed_data)\n",
        "\n",
        "    print('Step 6/8 ==> Extracting Transcript Features Excuted Succesfully \\n')\n",
        "\n",
        "    self.tfidfSimilarityCalculator()\n",
        "\n",
        "  def tfidfSimilarityCalculator(self):\n",
        "\n",
        "    print('Step 7/8 ==> Started ............')\n",
        "\n",
        "    # Loading the fitted Tf-Idf-Vectorizer Models\n",
        "\n",
        "    resume_Jd_filename = \"r&j.joblib\"\n",
        "    resume_Transcript_filename = \"r&t.joblib\"\n",
        "    Transcript_JD_filename = \"t&j.joblib\"\n",
        "\n",
        "    resume_Jd_vectorizer = joblib.load(resume_Jd_filename)\n",
        "    resume_Transcript_vectorizer = joblib.load(resume_Transcript_filename)\n",
        "    Transcript_JD_vectorizer = joblib.load(Transcript_JD_filename)\n",
        "\n",
        "    #  Transform new text data\n",
        "    def similarity_calculator(vectorizer , col1 , col2):\n",
        "\n",
        "      col1_vector = resume_Jd_vectorizer.transform(self.data[col1]).toarray()\n",
        "      col2_vector = resume_Jd_vectorizer.transform(self.data[col2]).toarray()\n",
        "\n",
        "      similarities = [\n",
        "          cosine_similarity(col1_vector[i].reshape(1, -1), col2_vector[i].reshape(1, -1))[0][0]\n",
        "          for i in range(len(col1_vector))]\n",
        "      return similarities\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    self.processed_data['Resume_Jd_similarity'] = similarity_calculator(resume_Jd_vectorizer , 'Resume_processed' , 'Job_Description_processed')\n",
        "    self.processed_data['Resume_Transcript_similarity'] = similarity_calculator(resume_Transcript_vectorizer , 'Resume_processed' , 'Transcript_processed')\n",
        "    self.processed_data['Transcript_Jd_similarity'] = similarity_calculator(Transcript_JD_vectorizer , 'Transcript_processed' , 'Job_Description_processed')\n",
        "\n",
        "    print('Step 7/8 ==> Similarity Calculation Excuted Succesfully \\n')\n",
        "\n",
        "    self.bertEmbeddings()\n",
        "\n",
        "\n",
        "  def bertEmbeddings(self):\n",
        "\n",
        "    print('Step 8/8 ==> Started ............')\n",
        "\n",
        "    transcripts = self.processed_data['Transcript'].tolist()\n",
        "    resumes = self.processed_data['Resume'].tolist()\n",
        "    job_desc = self.processed_data['Job Description'].tolist()\n",
        "\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "    def get_bert_embeddings_batch(texts, tokenizer, model, batch_size=32):\n",
        "      embeddings = []\n",
        "      total_batches = (len(texts) + batch_size - 1) // batch_size  # Total number of batches\n",
        "      for i in range(total_batches):\n",
        "          # Get the current batch\\n\",\n",
        "          batch = texts[i * batch_size:(i + 1) * batch_size]\n",
        "          inputs = tokenizer(batch, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "          with torch.no_grad():\n",
        "              outputs = model(**inputs)\n",
        "          # Use the [CLS] token representation for each text in the batch\\n\",\n",
        "          batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy(),\n",
        "          embeddings.extend(batch_embeddings)\n",
        "      return embeddings\n",
        "\n",
        "    tra_emb = get_bert_embeddings_batch(transcripts, tokenizer, model, batch_size=32)\n",
        "    flat_embeddings_tra = [embedding for batch in tra_emb for embedding in batch]\n",
        "    self.processed_data['Transcript_bert'] = flat_embeddings_tra\n",
        "\n",
        "    res_emb = get_bert_embeddings_batch(resumes, tokenizer, model, batch_size=32)\n",
        "    flat_embeddings_res = [embedding for batch in res_emb for embedding in batch]\n",
        "    self.processed_data['Resume_bert'] = flat_embeddings_res\n",
        "\n",
        "    jd_emb = get_bert_embeddings_batch(job_desc, tokenizer, model, batch_size=32)\n",
        "    flat_embeddings_jd = [embedding for batch in jd_emb for embedding in batch]\n",
        "    self.processed_data['Jd_bert'] = flat_embeddings_jd\n",
        "\n",
        "    trans_expanded = pd.DataFrame(self.processed_data['Transcript_bert'].tolist(), index=self.processed_data.index)\n",
        "    trans_expanded.columns = [f'trans_emb_{i}' for i in range(trans_expanded.shape[1])]\n",
        "\n",
        "    res_expanded = pd.DataFrame(self.processed_data['Resume_bert'].tolist(), index=self.processed_data.index)\n",
        "    res_expanded.columns = [f'resume_emb_{i}' for i in range(res_expanded.shape[1])]\n",
        "\n",
        "    jd_expanded = pd.DataFrame(self.processed_data['Jd_bert'].tolist(), index=self.processed_data.index)\n",
        "    jd_expanded.columns = [f'jd_emb_{i}' for i in range(jd_expanded.shape[1])]\n",
        "\n",
        "    self.processed_data = pd.concat([self.processed_data, trans_expanded, res_expanded , jd_expanded], axis=1)\n",
        "\n",
        "    simi_res_tra = []\n",
        "    simi_res_jd = []\n",
        "    simi_tra_jd = []\n",
        "\n",
        "    for tra , res in zip(flat_embeddings_tra , flat_embeddings_res):\n",
        "      simi_res_tra.append(cosine_similarity([tra],[res])[0][0])\n",
        "\n",
        "    for jd , res in zip(flat_embeddings_jd , flat_embeddings_res):\n",
        "      simi_res_jd.append(cosine_similarity([jd],[res])[0][0])\n",
        "\n",
        "    for tra , jd in zip(flat_embeddings_tra , flat_embeddings_jd):\n",
        "      simi_tra_jd.append(cosine_similarity([tra],[jd])[0][0])\n",
        "\n",
        "    self.processed_data['Bert_TransRes_Similarity']     = simi_res_tra\n",
        "    self.processed_data['Bert_ResJobDesc_Similarity']   = simi_res_jd\n",
        "    self.processed_data['Bert_TransJobDesc_Similarity'] = simi_tra_jd\n",
        "\n",
        "    self.processed_data.drop(columns=['Transcript_bert', 'Resume_bert','Jd_bert'], inplace = True)\n",
        "\n",
        "    print('Step 8/8 ==> BERT Embeddings Excuted Succesfully \\n')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFFwfGOk_Baa",
        "outputId": "b4ccaf2e-c024-4cf6-e3ce-4fb74eecc51f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initiating the Pre processing \n",
            "\n",
            "Step 1/8 ==> Started ............\n",
            "Step 1/8 ==> Pre processing of texts in the dataset executed Successfully \n",
            "\n",
            "Step 2/8 ==> Started ............\n",
            "Step 2/8 ==> Sentiment Calculation of Each colum Excuted Succesfully \n",
            "\n",
            "Step 3/8 ==> Started ............\n",
            "Step 3/8 ==> Extracting Word Counts Excuted Succesfully \n",
            "\n",
            "Step 4/8 ==> Started ............\n",
            "Step 4/8 ==> Extracting Years of Experience Excuted Succesfully \n",
            "\n",
            "Step 5/8 ==> Started ............\n",
            "Step 5/8 ==> Extracting Resume Features Excuted Succesfully \n",
            "\n",
            "Step 6/8 ==> Started ............\n",
            "Step 6/8 ==> Extracting Transcript Features Excuted Succesfully \n",
            "\n",
            "Step 7/8 ==> Started ............\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.6.1 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.6.1 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 7/8 ==> Similarity Calculation Excuted Succesfully \n",
            "\n",
            "Step 8/8 ==> Started ............\n",
            "Step 8/8 ==> BERT Embeddings Excuted Succesfully \n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_copy = df.copy()\n",
        "pre_processing = Preprocessing(df_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "UjSAhnKA_BUA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ],
      "source": [
        "pre_processed_data = pre_processing.processed_data\n",
        "xgb_file = 'xgb_model.joblib'\n",
        "nn_file = 'ann_model.h5'\n",
        "\n",
        "xgb_model = joblib.load(xgb_file)\n",
        "\n",
        "# Load the saved model\n",
        "ann_model = load_model(nn_file)\n",
        "\n",
        "def xgb_preds(model , data):\n",
        "  preds = model.predict_proba(data)[:, 1]\n",
        "  return preds\n",
        "\n",
        "def ann_preds(model , data):\n",
        "  preds = model.predict(data)\n",
        "  preds = [i[0] for i in preds]\n",
        "  return preds\n",
        "\n",
        "derived_features = pre_processed_data[['Transcript_sentiment','Resume_sentiment', 'JobDescription_sentiment', 'Transcript_words',\n",
        "       'Resume_words', 'Years_Experience', 'resume_avg_word_length', 'skill_match_count','university_education_count', 'transcript_vocab_diversity',\n",
        "       'transcript_avg_sentence_length' , 'Bert_TransRes_Similarity', 'Bert_TransJobDesc_Similarity' , 'Bert_ResJobDesc_Similarity']]\n",
        "\n",
        "embedding_columns = [col for col in pre_processed_data.columns if col.startswith('trans_emb_') or col.startswith('resume_emb_') or col.startswith('jd_emb_')]\n",
        "X = pre_processed_data[embedding_columns]\n",
        "\n",
        "new_data = pd.concat([X,derived_features],axis=1)\n",
        "\n",
        "pred_xgb = xgb_preds(xgb_model,new_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVI4E6wBxVEI",
        "outputId": "a167e2f7-b0be-432c-a76a-47c6ab5412d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 183ms/step\n"
          ]
        }
      ],
      "source": [
        "ann_pred = ann_preds(ann_model,new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "KjqtCAN_xU27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\heman\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.3.0 when using version 1.5.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "test_df = pd.DataFrame()\n",
        "test_df['XGB_Pred'] = pred_xgb\n",
        "test_df['ANN_Pred'] = ann_pred\n",
        "\n",
        "test_df['Ensemble_prob'] = (test_df['XGB_Pred'] + test_df['ANN_Pred'])/2\n",
        "\n",
        "test_df['Ensemble_Pred'] = test_df['Ensemble_prob'].round()\n",
        "\n",
        "le = joblib.load('label_encoder.joblib')\n",
        "\n",
        "test_df['Decision'] = le.inverse_transform(test_df['Ensemble_Pred'].astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "hjxuqmytxT5T",
        "outputId": "95c36db9-ffa3-461e-811f-ed4919c02c73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Decision\n",
              "reject    52\n",
              "select    48\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df['Decision'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "AR28cU2zxT2i"
      },
      "outputs": [],
      "source": [
        "df['Decision'] = test_df['Decision']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "jCjOUMwKxTzS",
        "outputId": "ab080f8d-b739-4eb2-8aa3-054d405d6dcf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Name</th>\n",
              "      <th>Role</th>\n",
              "      <th>Transcript</th>\n",
              "      <th>Resume</th>\n",
              "      <th>Reason for decision</th>\n",
              "      <th>Job Description</th>\n",
              "      <th>Decision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rivash0038</td>\n",
              "      <td>lahar singh</td>\n",
              "      <td>software engineer</td>\n",
              "      <td>**lahar singh: software engineer candidate int...</td>\n",
              "      <td>**lahar singh**\\n**software engineer candidate...</td>\n",
              "      <td>expected_experience : 9+ years, domains: e-com...</td>\n",
              "      <td>communicated ideas clearly and effectively., h...</td>\n",
              "      <td>select</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>benjry660</td>\n",
              "      <td>benjamin ryan</td>\n",
              "      <td>data engineer</td>\n",
              "      <td>interview transcript: data engineer position\\n...</td>\n",
              "      <td>here's a sample resume for benjamin ryan apply...</td>\n",
              "      <td>cultural fit</td>\n",
              "      <td>we are looking for a skilled data engineer wit...</td>\n",
              "      <td>reject</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rivash0968</td>\n",
              "      <td>amisha bedi</td>\n",
              "      <td>data scientist</td>\n",
              "      <td>**interview transcript: amisha bedi, data scie...</td>\n",
              "      <td>**candidate profile: amisha bedi**\\n\\n**role:*...</td>\n",
              "      <td>expected_experience : 6-8 years, domains: heal...</td>\n",
              "      <td>lacked key technical skills for the role., nee...</td>\n",
              "      <td>reject</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rivash0623</td>\n",
              "      <td>kairav mishra</td>\n",
              "      <td>product manager</td>\n",
              "      <td>**interview transcript: product manager positi...</td>\n",
              "      <td>**kairav mishra: product manager**\\n\\nas a sea...</td>\n",
              "      <td>expected_experience : 6-8 years, domains: tech...</td>\n",
              "      <td>had impressive experience and qualifications.,...</td>\n",
              "      <td>select</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bradgr792</td>\n",
              "      <td>bradley gross</td>\n",
              "      <td>product manager</td>\n",
              "      <td>product manager interview transcript\\n\\ninterv...</td>\n",
              "      <td>here's a sample resume for bradley gross apply...</td>\n",
              "      <td>cultural fit</td>\n",
              "      <td>we are looking for a skilled product manager w...</td>\n",
              "      <td>select</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ID           Name               Role  \\\n",
              "0  rivash0038    lahar singh  software engineer   \n",
              "1   benjry660  benjamin ryan      data engineer   \n",
              "2  rivash0968    amisha bedi     data scientist   \n",
              "3  rivash0623  kairav mishra    product manager   \n",
              "4   bradgr792  bradley gross    product manager   \n",
              "\n",
              "                                          Transcript  \\\n",
              "0  **lahar singh: software engineer candidate int...   \n",
              "1  interview transcript: data engineer position\\n...   \n",
              "2  **interview transcript: amisha bedi, data scie...   \n",
              "3  **interview transcript: product manager positi...   \n",
              "4  product manager interview transcript\\n\\ninterv...   \n",
              "\n",
              "                                              Resume  \\\n",
              "0  **lahar singh**\\n**software engineer candidate...   \n",
              "1  here's a sample resume for benjamin ryan apply...   \n",
              "2  **candidate profile: amisha bedi**\\n\\n**role:*...   \n",
              "3  **kairav mishra: product manager**\\n\\nas a sea...   \n",
              "4  here's a sample resume for bradley gross apply...   \n",
              "\n",
              "                                 Reason for decision  \\\n",
              "0  expected_experience : 9+ years, domains: e-com...   \n",
              "1                                       cultural fit   \n",
              "2  expected_experience : 6-8 years, domains: heal...   \n",
              "3  expected_experience : 6-8 years, domains: tech...   \n",
              "4                                       cultural fit   \n",
              "\n",
              "                                     Job Description Decision  \n",
              "0  communicated ideas clearly and effectively., h...   select  \n",
              "1  we are looking for a skilled data engineer wit...   reject  \n",
              "2  lacked key technical skills for the role., nee...   reject  \n",
              "3  had impressive experience and qualifications.,...   select  \n",
              "4  we are looking for a skilled product manager w...   select  "
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head() ## For prediction Data added Decision column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "send_data.to_excel('reslt.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Email sending failed: (535, b'5.7.8 Username and Password not accepted. For more information, go to\\n5.7.8  https://support.google.com/mail/?p=BadCredentials d2e1a72fcca58-72f8a6b31c9sm7909572b3a.57 - gsmtp')\n"
          ]
        }
      ],
      "source": [
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "import os\n",
        "\n",
        "class EmailSender:\n",
        "    def __init__(self, provider='gmail'):  # Corrected the constructor name\n",
        "        self.providers = {\n",
        "            'gmail': {\n",
        "                'smtp_server': 'smtp.gmail.com',\n",
        "                'port': 587\n",
        "            },\n",
        "            'outlook': {\n",
        "                'smtp_server': 'smtp-mail.outlook.com',\n",
        "                'port': 587\n",
        "            },\n",
        "            'yahoo': {\n",
        "                'smtp_server': 'smtp.mail.yahoo.com',\n",
        "                'port': 587\n",
        "            }\n",
        "        }\n",
        "        self.provider = provider\n",
        "\n",
        "    def send_email(self, sender_email, sender_password, to_email, subject, body, file_path=None):\n",
        "        try:\n",
        "            # Configure SMTP server details\n",
        "            smtp_server = self.providers[self.provider]['smtp_server']\n",
        "            port = self.providers[self.provider]['port']\n",
        "\n",
        "            # Create message\n",
        "            message = MIMEMultipart()\n",
        "            message[\"From\"] = sender_email\n",
        "            message[\"To\"] = to_email\n",
        "            message[\"Subject\"] = subject\n",
        "            message.attach(MIMEText(body, \"plain\"))\n",
        "\n",
        "            # Attach file if provided\n",
        "            if file_path and os.path.exists(file_path):\n",
        "                with open(file_path, \"rb\") as attachment:\n",
        "                    part = MIMEBase(\"application\", \"octet-stream\")\n",
        "                    part.set_payload(attachment.read())\n",
        "\n",
        "                encoders.encode_base64(part)\n",
        "                part.add_header(\n",
        "                    \"Content-Disposition\",\n",
        "                    f\"attachment; filename={os.path.basename(file_path)}\",\n",
        "                )\n",
        "                message.attach(part)\n",
        "\n",
        "            # Send email\n",
        "            with smtplib.SMTP(smtp_server, port) as server:\n",
        "                server.starttls()\n",
        "                server.login(sender_email, sender_password)\n",
        "                server.send_message(message)\n",
        "                print(\"Email sent successfully!\")\n",
        "                return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Email sending failed: {e}\")\n",
        "            return False\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    email_sender = EmailSender(provider='gmail')  # Can change to 'outlook' or 'yahoo'\n",
        "    email_sender.send_email(\n",
        "        sender_email=\"testridr@gmail.com\",\n",
        "        sender_password=\"123456\",\n",
        "        to_email=\"Sainaidukota281302@gmai.com\",\n",
        "        subject=\"Your Subject Here\",  # Add a subject\n",
        "        body=\"Your email body here\",  # Add a body\n",
        "        file_path=\"reslt.xlsx\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to log in...\n",
            "Login successful! Sending email...\n",
            "Email sent successfully!\n"
          ]
        }
      ],
      "source": [
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "import os\n",
        "\n",
        "class EmailSender:\n",
        "    def __init__(self, provider='gmail'):\n",
        "        self.providers = {\n",
        "            'gmail': {\n",
        "                'smtp_server': 'smtp.gmail.com',\n",
        "                'port': 587\n",
        "            },\n",
        "            'outlook': {\n",
        "                'smtp_server': 'smtp-mail.outlook.com',\n",
        "                'port': 587\n",
        "            },\n",
        "            'yahoo': {\n",
        "                'smtp_server': 'smtp.mail.yahoo.com',\n",
        "                'port': 587\n",
        "            }\n",
        "        }\n",
        "        self.provider = provider\n",
        "\n",
        "    def send_email(self, sender_email, sender_password, to_email, subject, body, file_path=None):\n",
        "        try:\n",
        "            # Configure SMTP server details\n",
        "            smtp_server = self.providers[self.provider]['smtp_server']\n",
        "            port = self.providers[self.provider]['port']\n",
        "\n",
        "            # Create message\n",
        "            message = MIMEMultipart()\n",
        "            message[\"From\"] = sender_email\n",
        "            message[\"To\"] = to_email\n",
        "            message[\"Subject\"] = subject\n",
        "            message.attach(MIMEText(body, \"plain\"))\n",
        "\n",
        "            # Attach file if provided\n",
        "            if file_path and os.path.exists(file_path):\n",
        "                with open(file_path, \"rb\") as attachment:\n",
        "                    part = MIMEBase(\"application\", \"octet-stream\")\n",
        "                    part.set_payload(attachment.read())\n",
        "\n",
        "                encoders.encode_base64(part)\n",
        "                part.add_header(\n",
        "                    \"Content-Disposition\",\n",
        "                    f\"attachment; filename={os.path.basename(file_path)}\",\n",
        "                )\n",
        "                message.attach(part)\n",
        "\n",
        "            # Send email\n",
        "            with smtplib.SMTP(smtp_server, port) as server:\n",
        "                server.starttls()\n",
        "                print(\"Attempting to log in...\")\n",
        "                server.login(sender_email, sender_password)\n",
        "                print(\"Login successful! Sending email...\")\n",
        "                server.send_message(message)\n",
        "                print(\"Email sent successfully!\")\n",
        "                return True\n",
        "\n",
        "        except smtplib.SMTPAuthenticationError as e:\n",
        "            print(f\"Authentication failed: {e}\")\n",
        "            print(\"Please check your email address and password.\")\n",
        "            print(\"If you have 2FA enabled, use an app password.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Email sending failed: {e}\")\n",
        "        return False\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    email_sender = EmailSender(provider='gmail')  # Can change to 'outlook' or 'yahoo'\n",
        "    email_sender.send_email(\n",
        "        sender_email=\"testridr@gmail.com\",\n",
        "        sender_password=\"lbyd mvus guaz nayu\",  # Use app password if 2FA is enabled\n",
        "        to_email=\"Sainaidukota281302@gmai.com\",\n",
        "        subject=\"Your Subject Here\",  # Add a subject\n",
        "        body=\"Your email body here\",  # Add a body\n",
        "        file_path=\"reslt.xlsx\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
